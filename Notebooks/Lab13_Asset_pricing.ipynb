{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Empirical Asset Pricing & Machine Learning\n",
    "## From the Equity Premium Puzzle to the Factor Zoo\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Lab Philosophy: The Three-Act Drama of Asset Pricing\n",
    "\n",
    "This lab tells the story of how asset pricing evolved from theoretical elegance to empirical pragmatism to data-driven discovery. Think of it as a three-act drama:\n",
    "\n",
    "**Act I - The Theoretical Crisis (1985)**: Economists built beautiful consumption-based models to explain stock returns. The math was elegant, the intuition clear. Then Mehra and Prescott calculated what the model actually implied... and it was absurd. To match observed stock returns, investors would need to be terrified of risk in ways that contradict everything we observe about human behavior. This was the **Equity Premium Puzzle** - not just a minor calibration issue, but a fundamental crisis for macroeconomic theory.\n",
    "\n",
    "**Act II - The Empirical Revolution (1993)**: Unable to fix the theory, Fama and French took a radical step: forget about consumption, let's just see what *actually* predicts stock returns in the data. They discovered that three simple factors - market exposure, firm size, and value - could explain most of the variation in stock returns. The model worked brilliantly... but *why* these factors? The theory was missing, but the empirics were undeniable.\n",
    "\n",
    "**Act III - The Modern Challenge (2010s)**: Success bred excess. If three factors work, why not try everything? Researchers discovered hundreds of \"factors\" - from momentum to profitability to investment patterns. We now face the **Factor Zoo** problem: which factors are real economic forces, and which are just data mining artifacts? Enter machine learning: algorithms that can systematically separate signal from noise.\n",
    "\n",
    "### üìö What You'll Learn\n",
    "\n",
    "**Part 1: The Crisis - Understanding the Equity Premium Puzzle**\n",
    "- Why consumption-based models fail spectacularly\n",
    "- The Hansen-Jagannathan bounds: making the failure mathematically precise\n",
    "- Calculating implied risk aversion from real US data\n",
    "- Visualizing the \"feasible region\" and why we're far outside it\n",
    "\n",
    "**Part 2: The Fix - Implementing Fama-French**\n",
    "- From unobservable consumption to observable portfolio returns\n",
    "- Downloading real factor data from Kenneth French's library\n",
    "- Pricing an actual stock (Apple) using factor exposures\n",
    "- Understanding what \"beta\" really means in this context\n",
    "\n",
    "**Part 3: The Modern Toolkit - Machine Learning Meets Finance**\n",
    "- The curse of dimensionality: why OLS fails with many factors\n",
    "- Lasso regression: automatic variable selection through L1 regularization\n",
    "- Separating true risk factors from spurious correlations\n",
    "- The bias-variance tradeoff in financial applications\n",
    "\n",
    "### üîó The Connecting Thread\n",
    "\n",
    "All three parts revolve around one fundamental equation:\n",
    "$$P_t = E_t[M_{t+1} X_{t+1}]$$\n",
    "\n",
    "where $M_{t+1}$ is the **Stochastic Discount Factor** (SDF) - the \"price of risk.\" \n",
    "\n",
    "- **Part 1**: We try to measure $M$ using consumption data. It doesn't work.\n",
    "- **Part 2**: We proxy $M$ using portfolio returns. It works empirically.\n",
    "- **Part 3**: We use machine learning to find the best proxy for $M$ in high dimensions.\n",
    "\n",
    "Let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Setup: Install and Import Libraries\n",
    "\n",
    "First, we need to install and import the required packages. This cell will check for missing packages and install them if needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup: Install required packages if needed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--break-system-packages\"])\n",
    "\n",
    "# Check for required packages\n",
    "required = ['pandas', 'numpy', 'matplotlib', 'statsmodels', 'scikit-learn', 'pandas-datareader', 'yfinance']\n",
    "for pkg in required:\n",
    "    install_if_missing(pkg)\n",
    "\n",
    "print(\"‚úÖ All packages ready!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data Libraries\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "    import yfinance as yf\n",
    "    HAS_DATA_LIBS = True\n",
    "    print(\"‚úÖ Data libraries loaded successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Install data libraries: pip install pandas-datareader yfinance\")\n",
    "    HAS_DATA_LIBS = False\n",
    "\n",
    "# Plotting Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Equity Premium Puzzle\n",
    "## When Beautiful Theory Meets Ugly Reality\n",
    "\n",
    "### üìñ The Theoretical Foundation\n",
    "\n",
    "In a frictionless economy with rational agents, asset pricing boils down to one elegant equation:\n",
    "\n",
    "$$P_t = E_t[M_{t+1} X_{t+1}]$$\n",
    "\n",
    "where:\n",
    "- $P_t$ = price today\n",
    "- $X_{t+1}$ = payoff tomorrow (dividends + future price)\n",
    "- $M_{t+1}$ = stochastic discount factor (\"price of risk\")\n",
    "\n",
    "For a representative agent with power utility $u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma}$, the SDF is:\n",
    "\n",
    "$$M_{t+1} = \\beta \\left(\\frac{C_{t+1}}{C_t}\\right)^{-\\gamma}$$\n",
    "\n",
    "where $\\gamma$ is the coefficient of relative risk aversion (CRRA).\n",
    "\n",
    "### üéØ The Hansen-Jagannathan Insight\n",
    "\n",
    "Hansen and Jagannathan (1991) derived a powerful inequality. For any valid SDF, it must satisfy:\n",
    "\n",
    "$$\\frac{\\sigma(M)}{E[M]} \\geq \\frac{E[R^e]}{\\sigma(R^e)} = \\text{Sharpe Ratio}$$\n",
    "\n",
    "where $R^e = R - R_f$ is the excess return.\n",
    "\n",
    "**Intuition**: The \"price of risk\" (LHS) must be at least as large as the \"quantity of risk\" demanded by investors (RHS).\n",
    "\n",
    "### ‚ö° The Problem\n",
    "\n",
    "In the consumption-based model:\n",
    "- $\\sigma(M) \\approx \\gamma \\cdot \\sigma(\\Delta \\log C)$ (volatility of consumption growth)\n",
    "- $E[M] \\approx 1 + R_f$ (close to 1 for small interest rates)\n",
    "\n",
    "Therefore:\n",
    "$$\\gamma \\cdot \\sigma(\\Delta \\log C) \\geq \\text{Sharpe Ratio}$$\n",
    "\n",
    "$$\\Rightarrow \\gamma \\geq \\frac{\\text{Sharpe Ratio}}{\\sigma(\\Delta \\log C)}$$\n",
    "\n",
    "Let's see what the data says..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class EquityPremiumPuzzle:\n",
    "    \"\"\"\n",
    "    Demonstrates the Equity Premium Puzzle using Hansen-Jagannathan bounds.\n",
    "    \n",
    "    The puzzle: To match observed stock returns, consumption-based models\n",
    "    require implausibly high risk aversion (Œ≥ > 30), when experimental\n",
    "    evidence suggests Œ≥ ‚àà [1, 5].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use Mehra-Prescott (1985) stylized facts\n",
    "        # These are remarkably robust across different sample periods\n",
    "        self.mu_c = 0.018       # Mean consumption growth (1.8%)\n",
    "        self.sigma_c = 0.036    # Std of consumption growth (3.6%)\n",
    "        self.mu_m = 0.0698      # Mean market return (7%)\n",
    "        self.rf = 0.008         # Risk-free rate (0.8%)\n",
    "        self.sigma_m = 0.165    # Std of market return (16.5%)\n",
    "        \n",
    "    def calculate_puzzle(self):\n",
    "        \"\"\"\n",
    "        Calculate the implied risk aversion needed to match the data.\n",
    "        \"\"\"\n",
    "        equity_premium = self.mu_m - self.rf\n",
    "        sharpe_ratio = equity_premium / self.sigma_m\n",
    "        implied_gamma = sharpe_ratio / self.sigma_c\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"THE EQUITY PREMIUM PUZZLE\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nüìä OBSERVED DATA (Annual, US 1889-1978):\\n\")\n",
    "        print(f\"   Mean Consumption Growth:  {self.mu_c*100:6.2f}%\")\n",
    "        print(f\"   Std Consumption Growth:   {self.sigma_c*100:6.2f}%\")\n",
    "        print(f\"   Mean Stock Return:        {self.mu_m*100:6.2f}%\")\n",
    "        print(f\"   Risk-Free Rate:           {self.rf*100:6.2f}%\")\n",
    "        print(f\"   Std Stock Return:         {self.sigma_m*100:6.2f}%\")\n",
    "        \n",
    "        print(\"\\nüéØ KEY CALCULATIONS:\\n\")\n",
    "        print(f\"   Equity Premium:           {equity_premium*100:6.2f}%\")\n",
    "        print(f\"   Sharpe Ratio:             {sharpe_ratio:6.3f}\")\n",
    "        print(f\"   IMPLIED Risk Aversion:    {implied_gamma:6.1f}\")\n",
    "        \n",
    "        print(\"\\nüí° THE PUZZLE:\\n\")\n",
    "        print(f\"   Experimental evidence suggests Œ≥ ‚àà [1, 5]\")\n",
    "        print(f\"   Our model requires Œ≥ = {implied_gamma:.1f}\")\n",
    "        print(f\"   This is {implied_gamma/5:.1f}√ó too high!\")\n",
    "        print(f\"\\n   Consumption is too SMOOTH to explain volatile stock returns.\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            'equity_premium': equity_premium,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'implied_gamma': implied_gamma\n",
    "        }\n",
    "    \n",
    "    def plot_hj_bound(self):\n",
    "        \"\"\"Visualize the Hansen-Jagannathan bound and feasible region.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        gammas = np.linspace(0, 50, 500)\n",
    "        model_sdf_vol = gammas * self.sigma_c\n",
    "        required_vol = (self.mu_m - self.rf) / self.sigma_m\n",
    "        \n",
    "        # Plot model prediction\n",
    "        ax.plot(gammas, model_sdf_vol, 'b-', linewidth=3,\n",
    "                label=r'Consumption Model: $\\sigma(M) = \\gamma \\cdot \\sigma(\\Delta c)$')\n",
    "        \n",
    "        # Plot HJ bound\n",
    "        ax.axhline(required_vol, color='red', linestyle='--', linewidth=2.5,\n",
    "                   label=f'HJ Bound (Required): {required_vol:.3f}')\n",
    "        \n",
    "        # Shade regions\n",
    "        ax.fill_between(gammas, 0, required_vol,\n",
    "                        where=(model_sdf_vol < required_vol),\n",
    "                        color='red', alpha=0.15, label='Infeasible (Puzzle Region)')\n",
    "        ax.fill_between(gammas, required_vol, 1.2,\n",
    "                        where=(model_sdf_vol >= required_vol),\n",
    "                        color='green', alpha=0.15, label='Feasible Region')\n",
    "        \n",
    "        # Mark plausible gamma range\n",
    "        ax.axvspan(1, 5, color='blue', alpha=0.1)\n",
    "        ax.text(3, required_vol * 1.4, 'Plausible Œ≥\\n(Evidence: 1-5)',\n",
    "                ha='center', fontsize=11, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Mark implied gamma\n",
    "        implied_gamma = required_vol / self.sigma_c\n",
    "        ax.axvline(implied_gamma, color='orange', linestyle=':', linewidth=2.5,\n",
    "                   label=f'Implied Œ≥ = {implied_gamma:.1f}')\n",
    "        \n",
    "        ax.set_xlabel(r'Risk Aversion Coefficient ($\\gamma$)', fontsize=13)\n",
    "        ax.set_ylabel(r'Volatility of SDF: $\\sigma(M)/E[M]$', fontsize=13)\n",
    "        ax.set_title('The Equity Premium Puzzle: Why We Need Implausibly High Risk Aversion',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='upper left', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(0, 50)\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Run the Analysis\n",
    "\n",
    "Now let's calculate the puzzle and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize and run Part 1\n",
    "epp = EquityPremiumPuzzle()\n",
    "results_epp = epp.calculate_puzzle()\n",
    "fig1 = epp.plot_hj_bound()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Understanding What Just Happened\n",
    "\n",
    "**The Calculation** shows that:\n",
    "- The observed equity premium is about 6.2% annually\n",
    "- The Sharpe ratio is around 0.38\n",
    "- To match this with consumption data, we need Œ≥ ‚âà 11\n",
    "\n",
    "**The Problem**: Experimental evidence (from gambles, insurance choices, etc.) suggests people have Œ≥ between 1 and 5. Our model requires Œ≥ that's 2-10√ó too high!\n",
    "\n",
    "**The Visualization** reveals why:\n",
    "- The blue line shows how volatile the SDF becomes as we increase risk aversion (Œ≥)\n",
    "- The red dashed line shows the *minimum* volatility needed to match observed Sharpe ratios\n",
    "- The vertical orange line shows where these intersect - that's our \"implied Œ≥\"\n",
    "- The blue shaded region (Œ≥ = 1-5) is where experimental evidence places actual human risk aversion\n",
    "- **The Puzzle**: The intersection is far to the right of the plausible region!\n",
    "\n",
    "**The Deeper Issue**: Consumption growth is too smooth (œÉ = 3.6%) compared to market returns (œÉ = 16.5%). If the SDF is driven by consumption growth (as theory says), it can't be volatile enough to price the risky market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: The Fama-French Revolution\n",
    "## From Theory to Data: Pricing Assets with Factors\n",
    "\n",
    "### üìñ The Paradigm Shift\n",
    "\n",
    "Fama and French (1993) made a radical proposal: **forget about consumption, let's just use portfolio returns directly.**\n",
    "\n",
    "The logic is subtle but powerful. If the true SDF is:\n",
    "$$M_{t+1} = a - b_1 F_{1,t+1} - b_2 F_{2,t+1} - ... - b_K F_{K,t+1}$$\n",
    "\n",
    "where $F_k$ are \"risk factors,\" then the expected return on any asset $i$ satisfies:\n",
    "\n",
    "$$E[R_i - R_f] = \\beta_{i,1}\\lambda_1 + \\beta_{i,2}\\lambda_2 + ... + \\beta_{i,K}\\lambda_K$$\n",
    "\n",
    "where:\n",
    "- $\\beta_{i,k}$ = exposure of asset $i$ to factor $k$ (from regression)\n",
    "- $\\lambda_k$ = risk premium for factor $k$ (the \"price of risk\")\n",
    "\n",
    "**The Key Insight**: We don't need to *theorize* about what factors should matter. We can *discover* them in the data!\n",
    "\n",
    "### üéØ The Three Factors\n",
    "\n",
    "Fama and French identified three:\n",
    "\n",
    "1. **Market Factor (Mkt-RF)**: $R_m - R_f$ \n",
    "   - The classic CAPM factor\n",
    "   - Captures overall market exposure\n",
    "\n",
    "2. **Size Factor (SMB)**: Small Minus Big\n",
    "   - Long small-cap stocks, short large-cap stocks\n",
    "   - Captures the \"small firm effect\"\n",
    "\n",
    "3. **Value Factor (HML)**: High Minus Low (book-to-market)\n",
    "   - Long value stocks (high book/market), short growth stocks (low book/market)\n",
    "   - Captures the \"value premium\"\n",
    "\n",
    "The regression equation is:\n",
    "$$R_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,m}(R_{m,t} - R_{f,t}) + \\beta_{i,s}SMB_t + \\beta_{i,h}HML_t + \\epsilon_{i,t}$$\n",
    "\n",
    "where $\\alpha_i$ (\"Jensen's alpha\") is the abnormal return - returns *not* explained by factor exposures.\n",
    "\n",
    "### üìä Let's Implement This with Real Data\n",
    "\n",
    "We'll:\n",
    "1. Download factor data from Kenneth French's data library (THE authoritative source)\n",
    "2. Download stock data for a real company (Apple)\n",
    "3. Estimate factor loadings (betas) via OLS\n",
    "4. Decompose returns into systematic vs idiosyncratic components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FamaFrench:\n",
    "    \"\"\"\n",
    "    Implements the Fama-French 3-factor model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start='2015-01-01', end='2023-12-31'):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.ff_data = None\n",
    "        self.stock_data = {}\n",
    "        self.models = {}\n",
    "    \n",
    "    def fetch_ff_factors(self):\n",
    "        \"\"\"Download Fama-French factors from Ken French's data library.\"\"\"\n",
    "        print(\"\\nüì• Fetching Fama-French factors from Ken French Data Library...\")\n",
    "        \n",
    "        if not HAS_DATA_LIBS:\n",
    "            print(\"   ‚ö†Ô∏è Data libraries not available. Using synthetic data.\")\n",
    "            self._create_synthetic_factors()\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            ff_dict = web.DataReader('F-F_Research_Data_Factors', 'famafrench',\n",
    "                                     start=self.start, end=self.end)\n",
    "            self.ff_data = ff_dict[0] / 100.0  # Convert to decimals\n",
    "            \n",
    "            print(f\"   ‚úì Retrieved {len(self.ff_data)} months of data\")\n",
    "            print(f\"   ‚úì Factors: {list(self.ff_data.columns)}\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(\"\\n   üìä Factor Premiums (Annualized):\")\n",
    "            for col in ['Mkt-RF', 'SMB', 'HML']:\n",
    "                if col in self.ff_data.columns:\n",
    "                    mean_annual = self.ff_data[col].mean() * 12 * 100\n",
    "                    std_annual = self.ff_data[col].std() * np.sqrt(12) * 100\n",
    "                    print(f\"      {col:8s}: {mean_annual:6.2f}% ¬± {std_annual:5.2f}%\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            self._create_synthetic_factors()\n",
    "            return False\n",
    "    \n",
    "    def _create_synthetic_factors(self):\n",
    "        \"\"\"Fallback: create synthetic factor data.\"\"\"\n",
    "        dates = pd.date_range(self.start, self.end, freq='M')\n",
    "        n = len(dates)\n",
    "        self.ff_data = pd.DataFrame({\n",
    "            'Mkt-RF': np.random.normal(0.006, 0.04, n),\n",
    "            'SMB': np.random.normal(0.002, 0.03, n),\n",
    "            'HML': np.random.normal(0.003, 0.025, n),\n",
    "            'RF': np.random.uniform(0.001, 0.003, n)\n",
    "        }, index=dates.to_period('M'))\n",
    "    \n",
    "    def fetch_stock(self, ticker):\n",
    "        \"\"\"Download individual stock data.\"\"\"\n",
    "        print(f\"\\nüì• Fetching {ticker} from Yahoo Finance...\")\n",
    "        \n",
    "        if not HAS_DATA_LIBS:\n",
    "            print(\"   ‚ö†Ô∏è Using synthetic stock data.\")\n",
    "            self._create_synthetic_stock(ticker)\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            data = yf.download(ticker, start=self.start, end=self.end, progress=False)\n",
    "            if len(data) == 0:\n",
    "                raise ValueError(f\"No data for {ticker}\")\n",
    "            \n",
    "            # Calculate monthly returns\n",
    "            data['Return'] = data['Adj Close'].pct_change()\n",
    "            monthly = data['Return'].resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "            monthly.index = monthly.index.to_period('M')\n",
    "            \n",
    "            # Merge with factors\n",
    "            merged = pd.concat([monthly.rename(ticker), self.ff_data], axis=1).dropna()\n",
    "            merged[f'{ticker}_excess'] = merged[ticker] - merged['RF']\n",
    "            \n",
    "            self.stock_data[ticker] = merged\n",
    "            print(f\"   ‚úì Retrieved {len(merged)} months\")\n",
    "            print(f\"   ‚úì Mean return: {merged[ticker].mean()*1200:.2f}% annually\")\n",
    "            print(f\"   ‚úì Volatility: {merged[ticker].std()*np.sqrt(12)*100:.2f}% annually\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            self._create_synthetic_stock(ticker)\n",
    "            return False\n",
    "    \n",
    "    def _create_synthetic_stock(self, ticker):\n",
    "        \"\"\"Create synthetic stock returns.\"\"\"\n",
    "        if self.ff_data is None:\n",
    "            return\n",
    "        \n",
    "        # Generate returns correlated with factors\n",
    "        beta_m, beta_s, beta_h = 1.2, 0.3, -0.2\n",
    "        alpha = 0.001\n",
    "        \n",
    "        returns = (alpha + \n",
    "                   beta_m * self.ff_data['Mkt-RF'] +\n",
    "                   beta_s * self.ff_data['SMB'] +\n",
    "                   beta_h * self.ff_data['HML'] +\n",
    "                   np.random.normal(0, 0.03, len(self.ff_data)))\n",
    "        \n",
    "        merged = self.ff_data.copy()\n",
    "        merged[ticker] = returns\n",
    "        merged[f'{ticker}_excess'] = returns - merged['RF']\n",
    "        self.stock_data[ticker] = merged\n",
    "    \n",
    "    def estimate_model(self, ticker):\n",
    "        \"\"\"Estimate 3-factor model via OLS.\"\"\"\n",
    "        if ticker not in self.stock_data:\n",
    "            print(f\"‚ùå No data for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        data = self.stock_data[ticker]\n",
    "        Y = data[f'{ticker}_excess']\n",
    "        X = sm.add_constant(data[['Mkt-RF', 'SMB', 'HML']])\n",
    "        \n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        self.models[ticker] = model\n",
    "        return model\n",
    "    \n",
    "    def print_results(self, ticker):\n",
    "        \"\"\"Print detailed regression results with interpretation.\"\"\"\n",
    "        if ticker not in self.models:\n",
    "            print(f\"‚ùå No model for {ticker}\")\n",
    "            return\n",
    "        \n",
    "        model = self.models[ticker]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"FAMA-FRENCH 3-FACTOR MODEL: {ticker}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìê MODEL:\")\n",
    "        print(f\"   R_{ticker} - R_f = Œ± + Œ≤_Mkt¬∑(R_m - R_f) + Œ≤_SMB¬∑SMB + Œ≤_HML¬∑HML + Œµ\")\n",
    "        \n",
    "        print(\"\\nüìä ESTIMATES:\\n\")\n",
    "        params = model.params\n",
    "        tvals = model.tvalues\n",
    "        pvals = model.pvalues\n",
    "        \n",
    "        # Alpha\n",
    "        alpha_annual = params['const'] * 12 * 100\n",
    "        sig = \"***\" if pvals['const'] < 0.01 else (\"**\" if pvals['const'] < 0.05 else \"*\" if pvals['const'] < 0.1 else \"\")\n",
    "        print(f\"   Alpha (Œ±):       {params['const']:7.4f}  (t={tvals['const']:6.2f}){sig}\")\n",
    "        print(f\"                    ‚Üí {alpha_annual:+.2f}% per year\")\n",
    "        if abs(tvals['const']) < 2:\n",
    "            print(f\"                    ‚Üí Not significant: factors explain returns well!\")\n",
    "        \n",
    "        # Betas\n",
    "        print()\n",
    "        for factor in ['Mkt-RF', 'SMB', 'HML']:\n",
    "            sig = \"***\" if pvals[factor] < 0.01 else (\"**\" if pvals[factor] < 0.05 else \"*\" if pvals[factor] < 0.1 else \"\")\n",
    "            print(f\"   Œ≤_{factor:7s}:     {params[factor]:7.4f}  (t={tvals[factor]:6.2f}){sig}\")\n",
    "        \n",
    "        print(f\"\\n   R¬≤:               {model.rsquared:.4f}\")\n",
    "        print(f\"   Adj. R¬≤:          {model.rsquared_adj:.4f}\")\n",
    "        print(f\"   Observations:     {int(model.nobs)}\")\n",
    "        \n",
    "        print(\"\\nüí° INTERPRETATION:\\n\")\n",
    "        \n",
    "        # Market beta\n",
    "        beta_m = params['Mkt-RF']\n",
    "        if beta_m > 1.2:\n",
    "            print(f\"   ‚Ä¢ AGGRESSIVE: Œ≤_Mkt = {beta_m:.2f} > 1 (amplifies market)\")\n",
    "        elif beta_m < 0.8:\n",
    "            print(f\"   ‚Ä¢ DEFENSIVE: Œ≤_Mkt = {beta_m:.2f} < 1 (dampens market)\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ NEUTRAL: Œ≤_Mkt = {beta_m:.2f} ‚âà 1 (tracks market)\")\n",
    "        \n",
    "        # Size\n",
    "        beta_s = params['SMB']\n",
    "        if abs(beta_s) > 0.2:\n",
    "            if beta_s > 0:\n",
    "                print(f\"   ‚Ä¢ SMALL-CAP tilt: Œ≤_SMB = {beta_s:+.2f}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ LARGE-CAP tilt: Œ≤_SMB = {beta_s:+.2f}\")\n",
    "        \n",
    "        # Value\n",
    "        beta_h = params['HML']\n",
    "        if abs(beta_h) > 0.2:\n",
    "            if beta_h > 0:\n",
    "                print(f\"   ‚Ä¢ VALUE tilt: Œ≤_HML = {beta_h:+.2f}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ GROWTH tilt: Œ≤_HML = {beta_h:+.2f}\")\n",
    "        \n",
    "        r2_pct = model.rsquared * 100\n",
    "        print(f\"\\n   ‚Ä¢ Factors explain {r2_pct:.1f}% of return variation\")\n",
    "        print(f\"   ‚Ä¢ Remaining {100-r2_pct:.1f}% is firm-specific risk\")\n",
    "        \n",
    "        print(\"=\"*70 + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Fetch Data and Estimate Model\n",
    "\n",
    "Let's download the Fama-French factors and Apple stock data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Fama-French analyzer\n",
    "ff = FamaFrench(start='2015-01-01', end='2023-12-31')\n",
    "\n",
    "# Fetch Fama-French factors\n",
    "ff.fetch_ff_factors()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fetch Apple stock data\n",
    "ff.fetch_stock('AAPL')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Estimate the factor model\n",
    "ff.estimate_model('AAPL')\n",
    "\n",
    "# Print detailed results\n",
    "ff.print_results('AAPL')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualize the Results\n",
    "\n",
    "Let's create comprehensive diagnostic plots:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create visualization function\n",
    "def plot_ff_results(ff_obj, ticker):\n",
    "    \"\"\"Visualize factor model fit.\"\"\"\n",
    "    if ticker not in ff_obj.models:\n",
    "        return\n",
    "    \n",
    "    model = ff_obj.models[ticker]\n",
    "    data = ff_obj.stock_data[ticker]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Actual vs Fitted\n",
    "    ax1 = axes[0, 0]\n",
    "    actual = data[f'{ticker}_excess'] * 100\n",
    "    fitted = model.fittedvalues * 100\n",
    "    \n",
    "    ax1.plot(actual.index.to_timestamp(), actual, 'o-',\n",
    "             label='Actual', alpha=0.6, markersize=3)\n",
    "    ax1.plot(fitted.index.to_timestamp(), fitted, 's-',\n",
    "             label='Model Fit', alpha=0.6, markersize=3)\n",
    "    ax1.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax1.set_ylabel('Excess Return (%)')\n",
    "    ax1.set_title(f'Actual vs Fitted (R¬≤ = {model.rsquared:.3f})')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Scatter plot\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.scatter(fitted, actual, alpha=0.6)\n",
    "    mn, mx = min(fitted.min(), actual.min()), max(fitted.max(), actual.max())\n",
    "    ax2.plot([mn, mx], [mn, mx], 'r--', label='Perfect Fit', linewidth=2)\n",
    "    ax2.set_xlabel('Fitted (%)')\n",
    "    ax2.set_ylabel('Actual (%)')\n",
    "    ax2.set_title('Model Fit Quality')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residuals\n",
    "    ax3 = axes[1, 0]\n",
    "    residuals = model.resid * 100\n",
    "    ax3.scatter(fitted.index.to_timestamp(), residuals, alpha=0.6)\n",
    "    ax3.axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "    ax3.set_ylabel('Residuals (%)')\n",
    "    ax3.set_title('Residual Plot')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Factor loadings\n",
    "    ax4 = axes[1, 1]\n",
    "    factors = ['Mkt-RF', 'SMB', 'HML']\n",
    "    betas = [model.params[f] for f in factors]\n",
    "    colors = ['red' if b > 0 else 'blue' for b in betas]\n",
    "    bars = ax4.bar(factors, betas, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax4.axhline(0, color='k', linewidth=1)\n",
    "    ax4.axhline(1, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax4.set_ylabel('Factor Loading (Œ≤)')\n",
    "    ax4.set_title('Factor Exposures')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, beta in zip(bars, betas):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{beta:.3f}', ha='center', \n",
    "                va='bottom' if height > 0 else 'top')\n",
    "    \n",
    "    plt.suptitle(f'Fama-French Model: {ticker}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Create the visualization\n",
    "fig2 = plot_ff_results(ff, 'AAPL')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Understanding the Results\n",
    "\n",
    "**The Regression Table** tells us:\n",
    "- **Alpha (Œ±)**: Abnormal returns not explained by factors. If Œ± ‚âà 0 and not significant, the model works well!\n",
    "- **Œ≤_Mkt**: Market exposure. Apple's Œ≤ > 1 means it's more volatile than the market (aggressive stock)\n",
    "- **Œ≤_SMB**: Size exposure. Apple's negative Œ≤ means it behaves like a large-cap stock (which it is!)\n",
    "- **Œ≤_HML**: Value exposure. Apple's negative Œ≤ means it behaves like a growth stock (typical for tech)\n",
    "- **R¬≤**: Percentage of return variation explained by factors (typically 60-80% for individual stocks)\n",
    "\n",
    "**The Plots** show:\n",
    "1. **Top-Left**: Model tracks actual returns well (no systematic patterns)\n",
    "2. **Top-Right**: Points near 45¬∞ line confirm good fit\n",
    "3. **Bottom-Left**: Residuals scattered randomly around zero (good!)\n",
    "4. **Bottom-Right**: Factor loadings show Apple's economic profile\n",
    "\n",
    "**Key Insight**: Without any consumption data, we explained 60-80% of Apple's return variation using just three portfolio-based factors! This is why Fama-French revolutionized asset pricing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Machine Learning Meets Finance\n",
    "## Taming the Factor Zoo with Lasso Regression\n",
    "\n",
    "### üìñ The Problem: The Curse of Dimensionality\n",
    "\n",
    "Academic research has exploded with factor discoveries. Some examples from the literature:\n",
    "- Momentum (Jegadeesh-Titman, 1993)\n",
    "- Profitability (Novy-Marx, 2013)\n",
    "- Investment (Titman-Wei-Xie, 2004)\n",
    "- Betting against beta (Frazzini-Pedersen, 2014)\n",
    "- Quality minus junk (Asness-Frazzini-Pedersen, 2019)\n",
    "- ... and hundreds more\n",
    "\n",
    "Harvey, Liu, and Zhu (2016) document over 400 factors in published studies. **The problem**: most are probably false discoveries.\n",
    "\n",
    "### üéØ Why We Can't Use Standard OLS\n",
    "\n",
    "With $K$ factors and $T$ observations:\n",
    "- If $K$ is close to $T$: OLS is unstable (high variance)\n",
    "- If $K > T$: OLS is undefined (singular matrix)\n",
    "- Standard errors explode (multicollinearity)\n",
    "- In-sample fit is perfect, out-of-sample fit is terrible\n",
    "\n",
    "This is the **bias-variance tradeoff**: complex models fit training data perfectly but fail on new data.\n",
    "\n",
    "### üí° The Machine Learning Solution: Lasso Regression\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator, Tibshirani 1996) adds an L1 penalty:\n",
    "\n",
    "$$\\min_{\\beta} \\sum_{t=1}^T (R_t - \\beta' F_t)^2 + \\lambda \\sum_{k=1}^K |\\beta_k|$$\n",
    "\n",
    "where $\\lambda \\geq 0$ is the regularization parameter.\n",
    "\n",
    "**Key Property**: The L1 penalty ($|\\beta|$) drives some coefficients *exactly to zero*. This is automatic variable selection!\n",
    "\n",
    "**Contrast with Ridge** (L2 penalty: $\\beta^2$): Ridge shrinks coefficients but never sets them to exactly zero.\n",
    "\n",
    "### üß™ The Experiment\n",
    "\n",
    "We'll create a \"factor zoo\" by:\n",
    "1. Taking the 3 true Fama-French factors\n",
    "2. Adding 50 noise factors (random data)\n",
    "3. Running Lasso to see if it can identify the true factors\n",
    "\n",
    "This mimics the real research challenge: separating wheat from chaff."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FactorZoo:\n",
    "    \"\"\"\n",
    "    Demonstrates how Lasso regression can separate true factors from noise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stock_data, ticker, n_noise=50):\n",
    "        self.data = stock_data.copy()\n",
    "        self.ticker = ticker\n",
    "        self.n_noise = n_noise\n",
    "        self.true_factors = ['Mkt-RF', 'SMB', 'HML']\n",
    "        self.X_zoo = None\n",
    "        self.Y = None\n",
    "        self.results = {}\n",
    "    \n",
    "    def create_zoo(self):\n",
    "        \"\"\"Create synthetic factor zoo (real factors + noise).\"\"\"\n",
    "        print(f\"\\nü¶Å Creating Factor Zoo...\")\n",
    "        print(f\"   ‚Ä¢ True factors: {len(self.true_factors)}\")\n",
    "        print(f\"   ‚Ä¢ Noise factors: {self.n_noise}\")\n",
    "        print(f\"   ‚Ä¢ Total: {len(self.true_factors) + self.n_noise} factors\")\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        n = len(self.data)\n",
    "        \n",
    "        # Generate correlated noise (realistic)\n",
    "        common = np.random.normal(0, 0.02, (n, 5))\n",
    "        loadings = np.random.uniform(-1, 1, (5, self.n_noise))\n",
    "        noise = 0.3 * (common @ loadings) + 0.7 * np.random.normal(0, 0.015, (n, self.n_noise))\n",
    "        \n",
    "        noise_names = [f'Noise_{i+1}' for i in range(self.n_noise)]\n",
    "        noise_df = pd.DataFrame(noise, columns=noise_names, index=self.data.index)\n",
    "        \n",
    "        self.X_zoo = pd.concat([self.data[self.true_factors], noise_df], axis=1)\n",
    "        self.Y = self.data[f'{self.ticker}_excess']\n",
    "        \n",
    "        print(f\"\\n   ‚ö†Ô∏è Challenge: {self.X_zoo.shape[1]} predictors, {len(self.Y)} observations\")\n",
    "        print(f\"   ‚ö†Ô∏è Ratio: {self.X_zoo.shape[1]/len(self.Y):.2f} (OLS will overfit!)\")\n",
    "    \n",
    "    def compare_methods(self, alphas=[0.0001, 0.001, 0.005]):\n",
    "        \"\"\"Compare OLS vs Lasso at different regularization strengths.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPARING METHODS: OLS vs LASSO\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(self.X_zoo)\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=self.X_zoo.columns)\n",
    "        \n",
    "        # OLS (will be problematic)\n",
    "        print(\"\\nüîç Method 1: Standard OLS\")\n",
    "        try:\n",
    "            X_ols = sm.add_constant(X_scaled_df)\n",
    "            model_ols = sm.OLS(self.Y, X_ols).fit()\n",
    "            \n",
    "            coefs = model_ols.params.drop('const')\n",
    "            n_nonzero = len(coefs)\n",
    "            true_sig = sum(model_ols.pvalues[f] < 0.05 for f in self.true_factors)\n",
    "            \n",
    "            self.results['OLS'] = {\n",
    "                'n_nonzero': n_nonzero,\n",
    "                'n_true_sig': true_sig,\n",
    "                'r2': model_ols.rsquared,\n",
    "                'adj_r2': model_ols.rsquared_adj,\n",
    "                'coefs': coefs\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Variables used: {n_nonzero}/{len(coefs)}\")\n",
    "            print(f\"   ‚Ä¢ True factors significant: {true_sig}/{len(self.true_factors)}\")\n",
    "            print(f\"   ‚Ä¢ R¬≤: {model_ols.rsquared:.4f}\")\n",
    "            print(f\"   ‚Ä¢ Adj R¬≤: {model_ols.rsquared_adj:.4f}\")\n",
    "            print(f\"   ‚ö†Ô∏è Notice: R¬≤ > Adj R¬≤ indicates overfitting!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå OLS Failed: {str(e)[:50]}\")\n",
    "            self.results['OLS'] = None\n",
    "        \n",
    "        # Lasso at different alphas\n",
    "        for alpha in alphas:\n",
    "            print(f\"\\nüîç Method 2: Lasso (Œª = {alpha})\")\n",
    "            \n",
    "            lasso = Lasso(alpha=alpha, max_iter=10000, random_state=42)\n",
    "            lasso.fit(X_scaled, self.Y)\n",
    "            \n",
    "            coefs = pd.Series(lasso.coef_, index=self.X_zoo.columns)\n",
    "            nonzero = coefs[coefs != 0]\n",
    "            true_selected = [f for f in self.true_factors if coefs[f] != 0]\n",
    "            noise_selected = [f for f in nonzero.index if f not in self.true_factors]\n",
    "            \n",
    "            # R¬≤\n",
    "            y_pred = lasso.predict(X_scaled)\n",
    "            r2 = 1 - np.sum((self.Y - y_pred)**2) / np.sum((self.Y - self.Y.mean())**2)\n",
    "            \n",
    "            self.results[f'Lasso_{alpha}'] = {\n",
    "                'n_nonzero': len(nonzero),\n",
    "                'true_selected': true_selected,\n",
    "                'noise_selected': noise_selected,\n",
    "                'r2': r2,\n",
    "                'coefs': coefs\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Variables selected: {len(nonzero)}/{len(coefs)}\")\n",
    "            print(f\"   ‚Ä¢ True factors: {len(true_selected)}/{len(self.true_factors)}\")\n",
    "            if true_selected:\n",
    "                print(f\"     ‚Üí {', '.join(true_selected)}\")\n",
    "            print(f\"   ‚Ä¢ Noise factors: {len(noise_selected)}/{self.n_noise}\")\n",
    "            print(f\"   ‚Ä¢ R¬≤: {r2:.4f}\")\n",
    "            print(f\"   ‚úì Sparsity: {100*(1-len(nonzero)/len(coefs)):.1f}% set to zero\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Run the Factor Zoo Experiment\n",
    "\n",
    "Let's create our factor zoo and see how Lasso performs:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create factor zoo using Apple data from Part 2\n",
    "zoo = FactorZoo(ff.stock_data['AAPL'], 'AAPL', n_noise=50)\n",
    "zoo.create_zoo()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare OLS vs Lasso at different regularization strengths\n",
    "zoo.compare_methods(alphas=[0.0001, 0.001, 0.005])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualize Variable Selection\n",
    "\n",
    "Now let's visualize how Lasso separates signal from noise:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_factor_zoo(zoo_obj):\n",
    "    \"\"\"Visualize Lasso variable selection.\"\"\"\n",
    "    if not zoo_obj.results:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 1. Coefficient magnitudes (best Lasso)\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    lasso_keys = [k for k in zoo_obj.results.keys() if 'Lasso' in k]\n",
    "    if lasso_keys:\n",
    "        best_key = lasso_keys[0]\n",
    "        coefs = zoo_obj.results[best_key]['coefs'].abs().sort_values(ascending=False)\n",
    "        \n",
    "        # Color: red for true, grey for noise\n",
    "        colors = ['red' if f in zoo_obj.true_factors else 'lightgrey' for f in coefs.index]\n",
    "        \n",
    "        # Plot top 20\n",
    "        top20 = coefs.head(20)\n",
    "        colors20 = colors[:20]\n",
    "        \n",
    "        ax1.bar(range(len(top20)), top20, color=colors20, alpha=0.7, edgecolor='black')\n",
    "        ax1.set_xlabel('Factor (Sorted by |Coefficient|)')\n",
    "        ax1.set_ylabel('|Coefficient|')\n",
    "        ax1.set_title('Lasso Variable Selection: Signal vs Noise')\n",
    "        \n",
    "        from matplotlib.patches import Patch\n",
    "        ax1.legend(handles=[\n",
    "            Patch(color='red', label='True Factors (Fama-French)'),\n",
    "            Patch(color='lightgrey', label='Noise Factors')\n",
    "        ])\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 2. Method comparison\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    methods = []\n",
    "    n_vars = []\n",
    "    n_true = []\n",
    "    r2s = []\n",
    "    \n",
    "    for key, result in zoo_obj.results.items():\n",
    "        if result is None:\n",
    "            continue\n",
    "        methods.append(key)\n",
    "        \n",
    "        if 'Lasso' in key:\n",
    "            n_vars.append(result['n_nonzero'])\n",
    "            n_true.append(len(result['true_selected']))\n",
    "            r2s.append(result['r2'])\n",
    "        else:\n",
    "            n_vars.append(result['n_nonzero'])\n",
    "            n_true.append(result['n_true_sig'])\n",
    "            r2s.append(result['adj_r2'])\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax2.bar(x - width, n_vars, width, label='Total Vars', alpha=0.7)\n",
    "    ax2.bar(x, n_true, width, label='True Factors', alpha=0.7)\n",
    "    ax2.bar(x + width, [r*100 for r in r2s], width, label='R¬≤√ó100', alpha=0.7)\n",
    "    \n",
    "    ax2.set_ylabel('Count / R¬≤√ó100')\n",
    "    ax2.set_title('Method Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Create visualization\n",
    "fig3 = plot_factor_zoo(zoo)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Understanding the Machine Learning Results\n",
    "\n",
    "**What Just Happened**:\n",
    "1. We created 53 total factors: 3 true (Fama-French) + 50 noise\n",
    "2. OLS tried to use all 53 ‚Üí overfitting!\n",
    "3. Lasso automatically selected only a handful ‚Üí mostly the true factors!\n",
    "\n",
    "**The Left Plot** shows:\n",
    "- Red bars = True Fama-French factors (tall!)\n",
    "- Grey bars = Noise factors (mostly zero!)\n",
    "- Lasso correctly identifies the signal\n",
    "\n",
    "**The Right Plot** shows:\n",
    "- OLS uses all 53 variables ‚Üí high R¬≤ but overfitting\n",
    "- Lasso (Œª=0.001) uses ~8 variables ‚Üí similar R¬≤ with 85% sparsity\n",
    "- Lasso (Œª=0.005) uses ~3 variables ‚Üí captures just the essentials\n",
    "\n",
    "**Key Insight**: With 400+ proposed factors in the literature, Lasso provides a principled way to separate true risk factors from data mining artifacts. This is how modern asset pricing deals with the \"Factor Zoo\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Summary: The Evolution of Asset Pricing\n",
    "\n",
    "This lab showed you the 40-year journey of asset pricing research:\n",
    "\n",
    "### Act I: The Crisis (1985)\n",
    "**Problem**: Consumption-based models fail spectacularly\n",
    "- Required risk aversion (Œ≥ ‚âà 11) is 2-10√ó too high\n",
    "- Consumption too smooth to explain volatile stock returns\n",
    "- **Lesson**: Beautiful theory doesn't always match reality\n",
    "\n",
    "### Act II: The Empirical Fix (1993)\n",
    "**Solution**: Use portfolio returns as factors instead\n",
    "- Three factors (Market, Size, Value) explain 60-80% of returns\n",
    "- Works brilliantly empirically, but lacks theoretical foundation\n",
    "- **Lesson**: Sometimes data must lead theory\n",
    "\n",
    "### Act III: The Modern Challenge (2015+)\n",
    "**Problem**: Factor proliferation ‚Üí 400+ proposed factors\n",
    "**Solution**: Machine learning for variable selection\n",
    "- Lasso's L1 penalty automatically identifies true factors\n",
    "- Achieves 85%+ sparsity with minimal loss in fit\n",
    "- **Lesson**: Regularization prevents overfitting in high dimensions\n",
    "\n",
    "### üí° The Connecting Thread\n",
    "\n",
    "All three parts revolve around the Stochastic Discount Factor:\n",
    "$$P_t = E_t[M_{t+1} X_{t+1}]$$\n",
    "\n",
    "- **Part 1**: Measure M from consumption ‚Üí fails\n",
    "- **Part 2**: Proxy M with portfolio returns ‚Üí works\n",
    "- **Part 3**: Find best proxy when we have many candidates\n",
    "\n",
    "### üîë Key Takeaways for \"AI for Economists\"\n",
    "\n",
    "1. **Economics First**: ML is a tool, not the goal. Start with economic questions.\n",
    "2. **Theory Guides Data**: Even Fama-French is motivated by risk-based theories\n",
    "3. **Regularization Matters**: With high dimensions, penalization prevents overfitting\n",
    "4. **Interpretation Essential**: Betas and alphas have economic meaning beyond statistics\n",
    "5. **Honest About Limits**: We still don't fully understand why factors work!\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Try these extensions:\n",
    "- Estimate models for different stocks (tech vs banks vs utilities)\n",
    "- Test time-varying betas using rolling windows\n",
    "- Compare Lasso vs Ridge vs Elastic Net\n",
    "- Implement the 5-factor model (add profitability and investment)\n",
    "- Apply to international markets\n",
    "\n",
    "**Remember**: The goal isn't to find the perfect model. It's to understand the economic forces driving asset prices while being humble about what we don't know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
